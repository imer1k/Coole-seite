{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XGBoost Regression für ENTSCHEIDUNGSDATUM\n",
        "\n",
        "Dieses Notebook erstellt ein vollständiges, reproduzierbares Regressions-Setup für die Zielvariable **ENTSCHEIDUNGSDATUM**.\n",
        "Es enthält EDA, sauberes Preprocessing, Baselines, XGBoost mit Early Stopping, Hyperparameter-Tuning und finale Evaluation inklusive Artefakt-Export."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup & Reproduzierbarkeit\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print('Imports geladen. RANDOM_STATE gesetzt auf', RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Daten laden & Quick-EDA\n",
        "\n",
        "Wir laden die CSV-Datei vom fest vorgegebenen Pfad. Falls die Datei fehlt, bricht das Notebook kontrolliert ab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_PATH = Path('/home/dsrg/Data/df_encoded_no_user_encode_original_Datum.csv')\n",
        "\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(f'Datei nicht gefunden: {DATA_PATH}. Bitte Pfad prüfen.')\n",
        "\n",
        "print('Lade Daten von:', DATA_PATH)\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print('Shape:', df.shape)\n",
        "display(df.head())\n",
        "display(df.dtypes)\n",
        "display(df.describe(include='all'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target = 'ENTSCHEIDUNGSDATUM'\n",
        "\n",
        "missing_per_col = df.isna().sum().sort_values(ascending=False)\n",
        "missing_total = df.isna().sum().sum()\n",
        "print('Missing Values pro Spalte:')\n",
        "display(missing_per_col)\n",
        "print('Missing Values total:', missing_total)\n",
        "\n",
        "y = df[target]\n",
        "print('Zielvariable Min/Max:', y.min(), y.max())\n",
        "outside_range = ((y < 0) | (y > 365)).mean()\n",
        "print('Anteil außerhalb [0, 365]:', round(outside_range * 100, 2), '%')\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.hist(y, bins=30, color='#5b8def', edgecolor='white')\n",
        "plt.title('Histogramm ENTSCHEIDUNGSDATUM')\n",
        "plt.xlabel('ENTSCHEIDUNGSDATUM')\n",
        "plt.ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "is_int_like = False\n",
        "if pd.api.types.is_integer_dtype(y):\n",
        "    is_int_like = True\n",
        "elif pd.api.types.is_float_dtype(y):\n",
        "    is_int_like = np.allclose(y.dropna(), np.round(y.dropna()), atol=1e-6)\n",
        "print('Zielvariable integer-ähnlich:', is_int_like)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split in Train/Val/Test\n",
        "\n",
        "Wir verwenden einen 70/15/15 Split mit Shuffle und Random State 42."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "id_col = None\n",
        "if 'ID' in X.columns:\n",
        "    id_col = X['ID'].copy()\n",
        "    X = X.drop(columns=['ID'])\n",
        "    print('ID-Spalte entfernt aus Features, wird für Outputs behalten.')\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=RANDOM_STATE, shuffle=True\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, random_state=RANDOM_STATE, shuffle=True\n",
        ")\n",
        "\n",
        "print('Train/Val/Test Shapes:', X_train.shape, X_val.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Quality Checks\n",
        "\n",
        "Wir prüfen NaNs in den Splits und zeigen die Zielverteilungen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def report_missing(name, X_split, y_split):\n",
        "    print(f'Missing in {name}: X={X_split.isna().sum().sum()}, y={y_split.isna().sum()}')\n",
        "\n",
        "report_missing('Train', X_train, y_train)\n",
        "report_missing('Val', X_val, y_val)\n",
        "report_missing('Test', X_test, y_test)\n",
        "\n",
        "print('Feature counts:', X_train.shape[1], X_val.shape[1], X_test.shape[1])\n",
        "\n",
        "def describe_target(name, y_split):\n",
        "    print(f'{name} target stats:')\n",
        "    display(y_split.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9]))\n",
        "\n",
        "describe_target('Train', y_train)\n",
        "describe_target('Val', y_val)\n",
        "describe_target('Test', y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing (leakage-frei)\n",
        "\n",
        "Wir erkennen Feature-Typen automatisch und bauen eine Pipeline mit Imputern und One-Hot-Encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
        "\n",
        "print('Numeric features:', len(numeric_features))\n",
        "print('Categorical features:', len(categorical_features))\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median'))\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent'))\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baselines\n",
        "\n",
        "Wir vergleichen gegen Dummy-Regressoren (Mean/Median) sowie optional Ridge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(name, y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return {'model': name, 'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
        "\n",
        "baseline_results = []\n",
        "\n",
        "dummy_mean = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', DummyRegressor(strategy='mean'))\n",
        "])\n",
        "dummy_mean.fit(X_train, y_train)\n",
        "pred_val = dummy_mean.predict(X_val)\n",
        "baseline_results.append(evaluate_model('DummyMean', y_val, pred_val))\n",
        "\n",
        "dummy_median = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', DummyRegressor(strategy='median'))\n",
        "])\n",
        "dummy_median.fit(X_train, y_train)\n",
        "pred_val = dummy_median.predict(X_val)\n",
        "baseline_results.append(evaluate_model('DummyMedian', y_val, pred_val))\n",
        "\n",
        "ridge = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', Ridge(random_state=RANDOM_STATE))\n",
        "])\n",
        "ridge.fit(X_train, y_train)\n",
        "pred_val = ridge.predict(X_val)\n",
        "baseline_results.append(evaluate_model('Ridge', y_val, pred_val))\n",
        "\n",
        "baseline_df = pd.DataFrame(baseline_results)\n",
        "display(baseline_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XGBoost Regression mit Early Stopping\n",
        "\n",
        "Wir verwenden XGBRegressor mit Early Stopping. Für maximale Kompatibilität wird ein Fallback mit Callbacks genutzt, falls der direkte Fit-Parameter nicht verfügbar ist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_params = {\n",
        "    'n_estimators': 5000,\n",
        "    'learning_rate': 0.05,\n",
        "    'max_depth': 6,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'reg_alpha': 0.0,\n",
        "    'reg_lambda': 1.0,\n",
        "    'objective': 'reg:squarederror',\n",
        "    'random_state': RANDOM_STATE,\n",
        "    'n_jobs': -1\n",
        "}\n",
        "\n",
        "X_train_proc = preprocessor.fit_transform(X_train)\n",
        "X_val_proc = preprocessor.transform(X_val)\n",
        "\n",
        "xgb_model = XGBRegressor(**xgb_params)\n",
        "\n",
        "def fit_xgb_with_early_stopping(model, X_tr, y_tr, X_va, y_va):\n",
        "    try:\n",
        "        model.fit(\n",
        "            X_tr,\n",
        "            y_tr,\n",
        "            eval_set=[(X_va, y_va)],\n",
        "            early_stopping_rounds=100,\n",
        "            verbose=False\n",
        "        )\n",
        "        return model\n",
        "    except TypeError:\n",
        "        from xgboost.callback import EarlyStopping\n",
        "        model.fit(\n",
        "            X_tr,\n",
        "            y_tr,\n",
        "            eval_set=[(X_va, y_va)],\n",
        "            callbacks=[EarlyStopping(rounds=100, save_best=True)],\n",
        "            verbose=False\n",
        "        )\n",
        "        return model\n",
        "\n",
        "xgb_model = fit_xgb_with_early_stopping(xgb_model, X_train_proc, y_train, X_val_proc, y_val)\n",
        "\n",
        "val_pred = xgb_model.predict(X_val_proc)\n",
        "val_metrics = evaluate_model('XGB-Baseline', y_val, val_pred)\n",
        "print('XGB baseline metrics:', val_metrics)\n",
        "print('Best iteration:', getattr(xgb_model, 'best_iteration', None))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter-Tuning (RandomizedSearchCV)\n",
        "\n",
        "Wir nutzen RandomizedSearchCV mit MAE-Scoring und 5-facher CV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_distributions = {\n",
        "    'model__max_depth': [3, 4, 5, 6, 7, 8],\n",
        "    'model__learning_rate': [0.03, 0.05, 0.08],\n",
        "    'model__subsample': [0.7, 0.8, 0.9, 1.0],\n",
        "    'model__colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'model__reg_alpha': [0.0, 0.1, 0.5],\n",
        "    'model__reg_lambda': [0.5, 1.0, 1.5],\n",
        "}\n",
        "\n",
        "search_model = XGBRegressor(\n",
        "    n_estimators=2000,\n",
        "    objective='reg:squarederror',\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=1\n",
        ")\n",
        "\n",
        "search_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', search_model)\n",
        "])\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    search_pipeline,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=40,\n",
        "    scoring='neg_mean_absolute_error',\n",
        "    cv=cv,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print('Starte RandomizedSearchCV...')\n",
        "random_search.fit(X_train, y_train)\n",
        "print('Best params:', random_search.best_params_)\n",
        "print('Best CV MAE:', -random_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Finales XGBoost-Modell mit Early Stopping\n",
        "\n",
        "Wir trainieren ein finales Modell mit den besten Parametern und Early Stopping auf der Validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_params = {k.replace('model__', ''): v for k, v in random_search.best_params_.items()}\n",
        "\n",
        "final_xgb_params = {\n",
        "    'n_estimators': 5000,\n",
        "    'objective': 'reg:squarederror',\n",
        "    'random_state': RANDOM_STATE,\n",
        "    'n_jobs': -1,\n",
        "}\n",
        "final_xgb_params.update(best_params)\n",
        "\n",
        "preprocessor_final = preprocessor\n",
        "\n",
        "X_train_proc = preprocessor_final.fit_transform(X_train)\n",
        "X_val_proc = preprocessor_final.transform(X_val)\n",
        "\n",
        "final_xgb = XGBRegressor(**final_xgb_params)\n",
        "final_xgb = fit_xgb_with_early_stopping(final_xgb, X_train_proc, y_train, X_val_proc, y_val)\n",
        "\n",
        "best_iteration = getattr(final_xgb, 'best_iteration', None)\n",
        "print('Final model best_iteration:', best_iteration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Finale Evaluation auf Test\n",
        "\n",
        "Wir evaluieren das Modell auf dem Testset und berechnen Metriken vor/nach Clipping (und optionalem Runden)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test_proc = preprocessor_final.transform(X_test)\n",
        "y_pred = final_xgb.predict(X_test_proc)\n",
        "\n",
        "def clip_predictions(preds, min_val=0, max_val=365):\n",
        "    return np.clip(preds, min_val, max_val)\n",
        "\n",
        "def round_predictions(preds):\n",
        "    return np.round(preds)\n",
        "\n",
        "metrics_raw = evaluate_model('XGB-Test', y_test, y_pred)\n",
        "\n",
        "y_pred_clipped = clip_predictions(y_pred)\n",
        "metrics_clipped = evaluate_model('XGB-Test-Clipped', y_test, y_pred_clipped)\n",
        "\n",
        "metrics_rounded = None\n",
        "if is_int_like:\n",
        "    y_pred_rounded = round_predictions(y_pred_clipped)\n",
        "    metrics_rounded = evaluate_model('XGB-Test-Rounded', y_test, y_pred_rounded)\n",
        "\n",
        "display(pd.DataFrame([metrics_raw, metrics_clipped] + ([metrics_rounded] if metrics_rounded else [])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plots_dir = Path('./artifacts/plots')\n",
        "plots_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Predicted vs Actual Scatter\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.6, color='#7c4dff')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='black', linestyle='--')\n",
        "plt.title('Predicted vs Actual (Test)')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.tight_layout()\n",
        "plt.savefig(plots_dir / 'pred_vs_actual.png')\n",
        "plt.show()\n",
        "\n",
        "# Residual Histogram\n",
        "residuals = y_test - y_pred\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.hist(residuals, bins=30, color='#26c6da', edgecolor='white')\n",
        "plt.title('Residuals Histogram (Test)')\n",
        "plt.xlabel('Residual')\n",
        "plt.ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.savefig(plots_dir / 'residuals_hist.png')\n",
        "plt.show()\n",
        "\n",
        "# Optional: Error by bins\n",
        "bins = [0, 30, 60, 90, 120, 180, 240, 300, 365]\n",
        "bin_labels = [f'{bins[i]}-{bins[i+1]}' for i in range(len(bins)-1)]\n",
        "bin_idx = np.digitize(y_test, bins, right=True) - 1\n",
        "bin_idx = np.clip(bin_idx, 0, len(bin_labels)-1)\n",
        "\n",
        "bin_mae = []\n",
        "for i, label in enumerate(bin_labels):\n",
        "    mask = bin_idx == i\n",
        "    if mask.sum() == 0:\n",
        "        bin_mae.append(np.nan)\n",
        "    else:\n",
        "        bin_mae.append(mean_absolute_error(y_test[mask], y_pred[mask]))\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.bar(bin_labels, bin_mae, color='#ff7043')\n",
        "plt.title('MAE nach Ziel-Bins (Test)')\n",
        "plt.xlabel('Ziel-Bin')\n",
        "plt.ylabel('MAE')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(plots_dir / 'mae_by_bins.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modell & Outputs speichern\n",
        "\n",
        "Wir speichern die Pipeline sowie eine Ergebnis-CSV für das Testset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "artifacts_dir = Path('./artifacts')\n",
        "artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Pipeline mit Preprocessor + finalem Modell\n",
        "final_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor_final),\n",
        "    ('model', final_xgb)\n",
        "])\n",
        "\n",
        "pipeline_path = artifacts_dir / 'xgb_reg_pipeline.joblib'\n",
        "joblib.dump(final_pipeline, pipeline_path)\n",
        "print('Pipeline gespeichert unter:', pipeline_path)\n",
        "\n",
        "# Test-Predictions CSV\n",
        "results = pd.DataFrame({\n",
        "    'y_true': y_test.values,\n",
        "    'y_pred': y_pred,\n",
        "    'y_pred_clipped': y_pred_clipped,\n",
        "})\n",
        "\n",
        "if id_col is not None:\n",
        "    results.insert(0, 'ID', id_col.loc[results.index].values)\n",
        "\n",
        "results['error'] = results['y_pred_clipped'] - results['y_true']\n",
        "results['abs_error'] = np.abs(results['error'])\n",
        "\n",
        "preds_path = artifacts_dir / 'test_predictions.csv'\n",
        "results.to_csv(preds_path, index=False)\n",
        "print('Test-Predictions gespeichert unter:', preds_path)\n",
        "\n",
        "summary = {\n",
        "    'best_params': best_params,\n",
        "    'best_iteration': best_iteration,\n",
        "    'test_metrics_raw': metrics_raw,\n",
        "    'test_metrics_clipped': metrics_clipped,\n",
        "}\n",
        "if metrics_rounded:\n",
        "    summary['test_metrics_rounded'] = metrics_rounded\n",
        "\n",
        "print('Summary:')\n",
        "print(summary)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
